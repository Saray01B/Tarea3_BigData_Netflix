# üìä ANALISIS DE DATOS EN TIEMPO REAL CON SPARK STREAMING Y APACHE KAFKA

El an√°lisis de datos en tiempo real con Spark Streaming y Apache Kafka permite obtener resultados inmediatos a partir de informaci√≥n que se genera de manera constante.

Kafka Producer se encarga de enviar mensajes al topic, simulando la generaci√≥n continua de datos. Una vez transmitidos, Spark Streaming los consume en tiempo real, permitiendo su transformaci√≥n, limpieza y realizaci√≥n de c√°lculos estad√≠sticos sobre la informaci√≥n, facilitando la obtenci√≥n de resultados inmediatos.

 ## **Pasos para la instalaci√≥n y ejecuci√≥n**
 
### 1Ô∏è‚É£ Instalar librer√≠a de Kafka en Python
 c√≥digo
```python
pip install kafka-python
```
### 2Ô∏è‚É£ Descargar e instalar Apache Kafka
c√≥digo
```python
wget https://downloads.apache.org/kafka/3.7.2/kafka_2.12-3.7.2.tgz
tar -xzf kafka_2.12-3.7.2.tgz
sudo mv kafka_2.12-3.7.2 /opt/Kafka
```
### 3Ô∏è‚É£ Iniciar los servicios de Kafka

 c√≥digo
 ```python
# Iniciar ZooKeeper
sudo /opt/Kafka/bin/zookeeper-server-start.sh /opt/Kafka/config/zookeeper.properties &

# Iniciar Kafka Server
sudo /opt/Kafka/bin/kafka-server-start.sh /opt/Kafka/config/server.properties &
```
### 4Ô∏è‚É£ Crear un t√≥pico (topic) de Kafka con el nombre de sensor_data

C√≥digo
```python
/opt/Kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic sensor_data
```
### üíæ Script 1: kafka_producer.py
Crear el archivo
C√≥digo
```python
nano kafka_producer.py
```

Genera datos simulados y los env√≠a al t√≥pico sensor_data.
C√≥digo
```python
import time
import json
import random
from kafka import KafkaProducer

# Funci√≥n que genera datos de sensores simulados
def generate_sensor_data():
    return {
        "sensor_id": random.randint(1, 10),
        "temperature": round(random.uniform(20, 30), 2),
        "humidity": round(random.uniform(30, 70), 2),
        "timestamp": int(time.time())
    }

# Configuraci√≥n del productor de Kafka
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda x: json.dumps(x).encode('utf-8')
)

# Bucle infinito que env√≠a datos cada segundo
while True:
    sensor_data = generate_sensor_data()
    producer.send('sensor_data', value=sensor_data)
    print(f"Sent: {sensor_data}")
    time.sleep(1)
```
**üìç Ejecuci√≥n del productor:**
C√≥digo
```python
python3 kafka_producer.py
```
### ‚ö° Script 2: spark_streaming_consumer.py
Crear el archivo
C√≥digo
```python
nano kafka_producer.py
```

Lee los datos enviados a Kafka y calcula estad√≠sticas en tiempo real.

C√≥digo
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, window
from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, TimestampType

# Crear sesi√≥n de Spark
spark = SparkSession.builder.appName("KafkaSparkStreaming").getOrCreate()
spark.sparkContext.setLogLevel("WARN")

# Esquema de los datos recibidos
schema = StructType([
    StructField("sensor_id", IntegerType()),
    StructField("temperature", FloatType()),
    StructField("humidity", FloatType()),
    StructField("timestamp", TimestampType())
])

# Leer los datos del topic sensor_data
df = spark.readStream.format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "sensor_data") \
    .load()

# Parsear el JSON recibido
parsed_df = df.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*")

# Agrupar los datos por ventana de 1 minuto y calcular promedio
windowed_stats = parsed_df.groupBy(
    window(col("timestamp"), "1 minute"), col("sensor_id")
).agg({"temperature": "avg", "humidity": "avg"})

# Mostrar los resultados en consola
query = windowed_stats.writeStream.outputMode("complete").format("console").start()
query.awaitTermination()
```
**üìç Ejecuci√≥n del consumidor:**

C√≥digo
```python
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.6 spark_streaming_consumer.py
```
### üìä Visualizaci√≥n y An√°lisis
Los resultados se muestran en tiempo real en la consola de Spark.

Se puede monitorear el procesamiento accediendo desde tu navegador a:

C√≥digo
```python
http://192.168.18.143:4040
```
